{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalando dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w0BqRMVqAGK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !git clone https://github.com/LucaLemos/UFRPE_AprendizagemReforco\n",
        "    sys.path.append(\"/content/UFRPE_AprendizagemReforco\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqFlN1Cpqlc",
        "outputId": "dc2f6c29-5837-4551-b9b6-ec11f188bcb0"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "    !pip install gymnasium==1.0.0   # conferir se precisa\n",
        "    #!pip install tianshou # Para criar o Replay_Buffer\n",
        "    #!pip install d3rlpy==2.7.0\n",
        "    # clone repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Criando Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from util.algorithms import run_sarsa\n",
        "from util.network import ReplayBuffer, MLP, epsilon_greedy_qnet, calc_loss\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_SIZE = 200_000  # Tamanho do conjunto de dados (replay buffer)\n",
        "LEARNING_RATE = 1e-3  # Taxa de aprendizado para o otimizador\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "BATCH_SIZE = 128  # Tamanho do batch para treinamento da rede neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = []\n",
        "ENVS_REPLAY_BUFFER = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Básico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run sarsa in <TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>\n",
            "Episódio 0 terminou com recompensa 0.0 na transição 3\n",
            "Episódio 100 terminou com recompensa 0.0 na transição 1601\n",
            "Episódio 200 terminou com recompensa 0.0 na transição 3020\n",
            "Episódio 300 terminou com recompensa 0.0 na transição 4652\n",
            "Episódio 400 terminou com recompensa 0.0 na transição 6299\n",
            "Episódio 500 terminou com recompensa 0.0 na transição 7809\n",
            "Episódio 600 terminou com recompensa 0.0 na transição 9291\n",
            "Episódio 700 terminou com recompensa 0.0 na transição 10720\n",
            "Episódio 800 terminou com recompensa 0.0 na transição 12090\n",
            "Episódio 900 terminou com recompensa 0.0 na transição 13643\n",
            "Episódio 1000 terminou com recompensa 0.0 na transição 15112\n",
            "Episódio 1100 terminou com recompensa 0.0 na transição 16545\n",
            "Episódio 1200 terminou com recompensa 0.0 na transição 18010\n",
            "Episódio 1300 terminou com recompensa 0.0 na transição 19488\n",
            "Episódio 1400 terminou com recompensa 0.0 na transição 21061\n",
            "Episódio 1500 terminou com recompensa 0.0 na transição 22749\n",
            "Episódio 1600 terminou com recompensa 0.0 na transição 24004\n",
            "Episódio 1700 terminou com recompensa 0.0 na transição 25605\n",
            "Episódio 1800 terminou com recompensa 0.0 na transição 27024\n",
            "Episódio 1900 terminou com recompensa 0.0 na transição 28318\n",
            "Episódio 2000 terminou com recompensa 0.0 na transição 29745\n",
            "Episódio 2100 terminou com recompensa 0.0 na transição 31299\n",
            "Episódio 2200 terminou com recompensa 0.0 na transição 32678\n",
            "Episódio 2300 terminou com recompensa 0.0 na transição 34133\n",
            "Episódio 2400 terminou com recompensa 0.0 na transição 35590\n",
            "Episódio 2500 terminou com recompensa 0.0 na transição 37048\n",
            "Episódio 2600 terminou com recompensa 0.0 na transição 38431\n",
            "Episódio 2700 terminou com recompensa 0.0 na transição 39799\n",
            "Episódio 2800 terminou com recompensa 0.0 na transição 41285\n",
            "Episódio 2900 terminou com recompensa 0.0 na transição 42867\n",
            "Episódio 3000 terminou com recompensa 0.0 na transição 44304\n",
            "Episódio 3100 terminou com recompensa 0.0 na transição 45798\n",
            "Episódio 3200 terminou com recompensa 0.0 na transição 46867\n",
            "Episódio 3300 terminou com recompensa 0.0 na transição 48060\n",
            "Episódio 3400 terminou com recompensa 0.0 na transição 49400\n",
            "Episódio 3500 terminou com recompensa 1.0 na transição 50670\n",
            "Episódio 3600 terminou com recompensa 0.0 na transição 52042\n",
            "Episódio 3700 terminou com recompensa 0.0 na transição 53355\n",
            "Episódio 3800 terminou com recompensa 0.0 na transição 54486\n",
            "Episódio 3900 terminou com recompensa 0.0 na transição 55706\n",
            "Episódio 4000 terminou com recompensa 0.0 na transição 57110\n",
            "Episódio 4100 terminou com recompensa 0.0 na transição 58576\n",
            "Episódio 4200 terminou com recompensa 0.0 na transição 59909\n",
            "Episódio 4300 terminou com recompensa 0.0 na transição 61241\n",
            "Episódio 4400 terminou com recompensa 0.0 na transição 62636\n",
            "Episódio 4500 terminou com recompensa 0.0 na transição 64074\n",
            "Episódio 4600 terminou com recompensa 0.0 na transição 65292\n",
            "Episódio 4700 terminou com recompensa 0.0 na transição 66651\n",
            "Episódio 4800 terminou com recompensa 0.0 na transição 67962\n",
            "Episódio 4900 terminou com recompensa 0.0 na transição 69337\n",
            "Episódio 5000 terminou com recompensa 0.0 na transição 70705\n",
            "Episódio 5100 terminou com recompensa 1.0 na transição 72000\n",
            "Episódio 5200 terminou com recompensa 0.0 na transição 73355\n",
            "Episódio 5300 terminou com recompensa 0.0 na transição 74787\n",
            "Episódio 5400 terminou com recompensa 1.0 na transição 76101\n",
            "Episódio 5500 terminou com recompensa 0.0 na transição 77498\n",
            "Episódio 5600 terminou com recompensa 0.0 na transição 78887\n",
            "Episódio 5700 terminou com recompensa 0.0 na transição 80326\n",
            "Episódio 5800 terminou com recompensa 0.0 na transição 81604\n",
            "Episódio 5900 terminou com recompensa 0.0 na transição 82881\n",
            "Episódio 6000 terminou com recompensa 0.0 na transição 84276\n",
            "Episódio 6100 terminou com recompensa 0.0 na transição 85496\n",
            "Episódio 6200 terminou com recompensa 0.0 na transição 86880\n",
            "Episódio 6300 terminou com recompensa 0.0 na transição 88410\n",
            "Episódio 6400 terminou com recompensa 0.0 na transição 89747\n",
            "Episódio 6500 terminou com recompensa 1.0 na transição 91184\n",
            "Episódio 6600 terminou com recompensa 0.0 na transição 92549\n",
            "Episódio 6700 terminou com recompensa 1.0 na transição 93744\n",
            "Episódio 6800 terminou com recompensa 0.0 na transição 95197\n",
            "Episódio 6900 terminou com recompensa 0.0 na transição 96463\n",
            "Episódio 7000 terminou com recompensa 0.0 na transição 97786\n",
            "Episódio 7100 terminou com recompensa 0.0 na transição 99237\n",
            "Episódio 7200 terminou com recompensa 0.0 na transição 100504\n",
            "Episódio 7300 terminou com recompensa 0.0 na transição 101873\n",
            "Episódio 7400 terminou com recompensa 0.0 na transição 103087\n",
            "Episódio 7500 terminou com recompensa 1.0 na transição 104468\n",
            "Episódio 7600 terminou com recompensa 0.0 na transição 105948\n",
            "Episódio 7700 terminou com recompensa 0.0 na transição 107238\n",
            "Episódio 7800 terminou com recompensa 0.0 na transição 108602\n",
            "Episódio 7900 terminou com recompensa 0.0 na transição 109978\n",
            "Episódio 8000 terminou com recompensa 0.0 na transição 111329\n",
            "Episódio 8100 terminou com recompensa 0.0 na transição 112638\n",
            "Episódio 8200 terminou com recompensa 0.0 na transição 113985\n",
            "Episódio 8300 terminou com recompensa 0.0 na transição 115436\n",
            "Episódio 8400 terminou com recompensa 0.0 na transição 117021\n",
            "Episódio 8500 terminou com recompensa 0.0 na transição 118177\n",
            "Episódio 8600 terminou com recompensa 0.0 na transição 119499\n",
            "Episódio 8700 terminou com recompensa 0.0 na transição 120895\n",
            "Episódio 8800 terminou com recompensa 0.0 na transição 122368\n",
            "Episódio 8900 terminou com recompensa 0.0 na transição 123826\n",
            "Episódio 9000 terminou com recompensa 0.0 na transição 125275\n",
            "Episódio 9100 terminou com recompensa 0.0 na transição 126560\n",
            "Episódio 9200 terminou com recompensa 0.0 na transição 127650\n",
            "Episódio 9300 terminou com recompensa 0.0 na transição 128940\n",
            "Episódio 9400 terminou com recompensa 1.0 na transição 130183\n",
            "Episódio 9500 terminou com recompensa 0.0 na transição 131577\n",
            "Episódio 9600 terminou com recompensa 0.0 na transição 132950\n",
            "Episódio 9700 terminou com recompensa 0.0 na transição 134342\n",
            "Episódio 9800 terminou com recompensa 1.0 na transição 135523\n",
            "Episódio 9900 terminou com recompensa 0.0 na transição 136805\n",
            "Episódio 10000 terminou com recompensa 0.0 na transição 138079\n",
            "Episódio 10100 terminou com recompensa 0.0 na transição 139526\n",
            "Episódio 10200 terminou com recompensa 0.0 na transição 140925\n",
            "Episódio 10300 terminou com recompensa 0.0 na transição 142265\n",
            "Episódio 10400 terminou com recompensa 0.0 na transição 143548\n",
            "Episódio 10500 terminou com recompensa 1.0 na transição 145027\n",
            "Episódio 10600 terminou com recompensa 1.0 na transição 146321\n",
            "Episódio 10700 terminou com recompensa 0.0 na transição 147608\n",
            "Episódio 10800 terminou com recompensa 1.0 na transição 148908\n",
            "Episódio 10900 terminou com recompensa 0.0 na transição 150196\n",
            "Episódio 11000 terminou com recompensa 0.0 na transição 151466\n",
            "Episódio 11100 terminou com recompensa 0.0 na transição 152576\n",
            "Episódio 11200 terminou com recompensa 0.0 na transição 153907\n",
            "Episódio 11300 terminou com recompensa 0.0 na transição 155294\n",
            "Episódio 11400 terminou com recompensa 0.0 na transição 156460\n",
            "Episódio 11500 terminou com recompensa 0.0 na transição 157806\n",
            "Episódio 11600 terminou com recompensa 0.0 na transição 159130\n",
            "Episódio 11700 terminou com recompensa 0.0 na transição 160671\n",
            "Episódio 11800 terminou com recompensa 0.0 na transição 161938\n",
            "Episódio 11900 terminou com recompensa 0.0 na transição 163182\n",
            "Episódio 12000 terminou com recompensa 0.0 na transição 164580\n",
            "Episódio 12100 terminou com recompensa 0.0 na transição 165812\n",
            "Episódio 12200 terminou com recompensa 0.0 na transição 167272\n",
            "Episódio 12300 terminou com recompensa 0.0 na transição 168555\n",
            "Episódio 12400 terminou com recompensa 0.0 na transição 169840\n",
            "Episódio 12500 terminou com recompensa 0.0 na transição 171225\n",
            "Episódio 12600 terminou com recompensa 0.0 na transição 172370\n",
            "Episódio 12700 terminou com recompensa 0.0 na transição 173465\n",
            "Episódio 12800 terminou com recompensa 1.0 na transição 174894\n",
            "Episódio 12900 terminou com recompensa 0.0 na transição 176129\n",
            "Episódio 13000 terminou com recompensa 0.0 na transição 177455\n",
            "Episódio 13100 terminou com recompensa 0.0 na transição 178882\n",
            "Episódio 13200 terminou com recompensa 0.0 na transição 180155\n",
            "Episódio 13300 terminou com recompensa 0.0 na transição 181547\n",
            "Episódio 13400 terminou com recompensa 0.0 na transição 182954\n",
            "Episódio 13500 terminou com recompensa 0.0 na transição 184241\n",
            "Episódio 13600 terminou com recompensa 0.0 na transição 185459\n",
            "Episódio 13700 terminou com recompensa 0.0 na transição 186764\n",
            "Episódio 13800 terminou com recompensa 1.0 na transição 187951\n",
            "Episódio 13900 terminou com recompensa 0.0 na transição 189230\n",
            "Episódio 14000 terminou com recompensa 0.0 na transição 190533\n",
            "Episódio 14100 terminou com recompensa 0.0 na transição 191683\n",
            "Episódio 14200 terminou com recompensa 0.0 na transição 192950\n",
            "Episódio 14300 terminou com recompensa 0.0 na transição 194354\n",
            "Episódio 14400 terminou com recompensa 0.0 na transição 195515\n",
            "Episódio 14500 terminou com recompensa 0.0 na transição 196907\n",
            "Episódio 14600 terminou com recompensa 0.0 na transição 198179\n",
            "Episódio 14700 terminou com recompensa 1.0 na transição 199484\n",
            "\n",
            "Run sarsa in <TimeLimit<OrderEnforcing<PassiveEnvChecker<TaxiEnv<Taxi-v3>>>>>\n",
            "Episódio 0 terminou com recompensa -677 na transição 199\n",
            "Episódio 100 terminou com recompensa -263 na transição 19519\n",
            "Episódio 200 terminou com recompensa -245 na transição 38503\n",
            "Episódio 300 terminou com recompensa -164 na transição 57658\n",
            "Episódio 400 terminou com recompensa -272 na transição 76680\n",
            "Episódio 500 terminou com recompensa -308 na transição 95431\n",
            "Episódio 600 terminou com recompensa -263 na transição 114426\n",
            "Episódio 700 terminou com recompensa -50 na transição 133412\n",
            "Episódio 800 terminou com recompensa -272 na transição 152269\n",
            "Episódio 900 terminou com recompensa -263 na transição 170880\n",
            "Episódio 1000 terminou com recompensa -335 na transição 189799\n",
            "\n",
            "Run sarsa in <OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Episódio 0 terminou com recompensa -425 na transição 127\n",
            "Episódio 100 terminou com recompensa -1046 na transição 49350\n",
            "Episódio 200 terminou com recompensa -758 na transição 83951\n",
            "Episódio 300 terminou com recompensa -400 na transição 112318\n",
            "Episódio 400 terminou com recompensa -442 na transição 138537\n",
            "Episódio 500 terminou com recompensa -145 na transição 161003\n",
            "Episódio 600 terminou com recompensa -180 na transição 180308\n",
            "Episódio 700 terminou com recompensa -426 na transição 199177\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Passo 1: Coletar um conjunto fixo de transições (Replay Buffer)\n",
        "# Ambientes Básicos\n",
        "ENV_NAMES += [\"FrozenLake-v1\", \"Taxi-v3\", \"CliffWalking-v0\"]\n",
        "for i, env_name in enumerate(ENV_NAMES):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    replay_buffer = ReplayBuffer(DATASET_SIZE, BATCH_SIZE, device)\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    \n",
        "    sum_rewards_per_ep, q = run_sarsa(env, replay_buffer, DATASET_SIZE, LEARNING_RATE, GAMMA)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer, sum_rewards_per_ep, q))\n",
        "    replay_buffer.save_config(f\"config\\dataset\\sarsa\\{env_name}.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset com Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim, nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DQN_TRAIN(env, env_name, gamma, qnet, qnet_lr, target_qnet, target_update_freq, replay_buffer, batch_size, epsilon_f, epsilon_decay_period, transitions):\n",
        "    # Cria o otimizador, que vai fazer o ajuste dos pesos da 'qnet',\n",
        "    # Usa uma técnica de gradiente descendente de destaque, chamada ADAM\n",
        "    optimizer = optim.Adam(qnet.parameters(), lr=qnet_lr)\n",
        "\n",
        "    episode_reward_list = []\n",
        "    step = 0\n",
        "    epsilon = 1.0\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    for _ in range(transitions):\n",
        "        # Decaimento linear do epsilon\n",
        "        epsilon = max(epsilon_f, 1.0 - step / epsilon_decay_period)\n",
        "\n",
        "        action = epsilon_greedy_qnet(qnet, env, state, epsilon)\n",
        "\n",
        "        # Faz um passo / Aplica uma ação no ambiente\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        step += 1\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Adiciona no buffer\n",
        "        replay_buffer.add(state, action, reward, terminated, new_state)\n",
        "        state = new_state\n",
        "\n",
        "        if done:\n",
        "            episode_reward_list.append(episode_reward)\n",
        "            if len(episode_reward_list) % 100 == 0:\n",
        "                mean_reward = np.mean(episode_reward_list[-100:])\n",
        "                print(f'Episode Reward: {episode_reward_list[-1]} | Epsilon: {epsilon} | Step: {step} | Mean Reward: {mean_reward}')\n",
        "            state, _ = env.reset()\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            # Abaixo, faz vários loggings de dados\n",
        "            \n",
        "        # Faz a 'tgt_net' receber os mesmos valores de pesos da 'qnet', na frequência indicada\n",
        "        \n",
        "        if step > batch_size:\n",
        "            if step % target_update_freq == 0:\n",
        "                target_qnet.load_state_dict(qnet.state_dict())\n",
        "            # Escolhe amostras aleatórios do buffer e faz uma atualização dos pesos da rede\n",
        "            optimizer.zero_grad()\n",
        "            batch = replay_buffer.sample_continuous()\n",
        "            loss_t = calc_loss(batch, qnet, target_qnet, gamma)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "    mean_reward = np.mean(episode_reward_list[-100:])\n",
        "    print(f\"Mean_reward: {mean_reward}\")\n",
        "    filename = f\"./neural_dataset/{env_name}.pth\"\n",
        "    torch.save(qnet.state_dict(), filename)\n",
        "    print(f\"Model saved as {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GOAL_REWARD = 200\n",
        "GAMMA = 0.999\n",
        "REPLAY_SIZE = 100_000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "SYNC_TARGET_FRAMES = 1_000\n",
        "\n",
        "EPSILON_DECAY_PERIOD = 75_000\n",
        "EPSILON_FINAL = 2e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ENV_NAMES += [\"CartPole-v1\"]\n",
        "ENV_NAMES += [\"LunarLander-v3\"]\n",
        "for i, env_name in enumerate(ENV_NAMES[-1:]):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #replay_buffer = ReplayBuffer(DATASET_SIZE, BATCH_SIZE, device)\n",
        "    buffer = ReplayBuffer(DATASET_SIZE, BATCH_SIZE, device)\n",
        "    print(env_name)\n",
        "    env = gym.make(env_name)\n",
        "    \n",
        "    qnet1 = MLP(env.observation_space.shape[0], env.action_space.n, [256,256])\n",
        "    qtarget1 = MLP(env.observation_space.shape[0], env.action_space.n, [256,256])\n",
        "\n",
        "    DQN_TRAIN(\n",
        "    env = env,\n",
        "    env_name = env_name,\n",
        "    gamma = GAMMA,\n",
        "    qnet = qnet1,\n",
        "    qnet_lr = LEARNING_RATE,\n",
        "    target_qnet = qtarget1,\n",
        "    target_update_freq = SYNC_TARGET_FRAMES,\n",
        "    replay_buffer= buffer,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epsilon_f = EPSILON_FINAL,\n",
        "    epsilon_decay_period = EPSILON_DECAY_PERIOD,\n",
        "    transitions = REPLAY_SIZE)\n",
        "\n",
        "    #!sum_rewards_per_ep, q = run_sarsa(env, replay_buffer, DATASET_SIZE, LEARNING_RATE, GAMMA)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, buffer))\n",
        "    buffer.save_config(f\"config\\dataset\\dqn\\{env_name}.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinando o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregando Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from util.network import ReplayBuffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = [\"FrozenLake-v1\", \"Taxi-v3\", \"CliffWalking-v0\", \"CartPole-v1\", \"LunarLander-v3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER = []\n",
        "for env_name in ENV_NAMES[:3]:\n",
        "    replay_buffer = ReplayBuffer.load_config(f\"config\\dataset\\sarsa\\{env_name}.json\")\n",
        "    env = gym.make(env_name)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer))\n",
        "for env_name in ENV_NAMES[3:]:\n",
        "    replay_buffer = ReplayBuffer.load_config(f\"config\\dataset\\dqn\\{env_name}.json\")\n",
        "    env = gym.make(env_name)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definições de Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_config(env_name, count_episodes, seed, gamma, tau, lr, steps, hidden_size, isDiscrete):\n",
        "    parser = argparse.ArgumentParser(description='RL')\n",
        "    parser.add_argument(\"--run_name\", type=str, default=f\"{env_name}-DQN-FQI\", help=\"Run name, default: FQI-DQN\")\n",
        "    parser.add_argument(\"--env\", type=str, default=env_name, help=\"Gym environment name, default: CartPole-v0\")\n",
        "    parser.add_argument(\"--episodes\", type=int, default=count_episodes, help=\"Number of episodes, default: 200\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=seed, help=\"Seed, default: 1\")\n",
        "    parser.add_argument(\"--steps\", type=int, default=steps, help=\"Saves the network every x epochs, default: 25\")\n",
        "    \n",
        "    parser.add_argument(\"--gamma\", type=float, default=gamma, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--tau\", type=float, default=tau, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=lr, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--hidden_size\", type=list[int], default=hidden_size, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--isDiscrete\", type=bool, default=isDiscrete, help=\"Saves the network every x epochs, default: 25\")\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from util.network import FQIAgent, to_one_hot, MLP\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_DQN_FQI(config, buffer):\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "    env = gym.make(config.env)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    losses = deque(maxlen=1000)\n",
        "    if config.isDiscrete:\n",
        "        agent = FQIAgent(env.observation_space.n, env.action_space.n, config.tau, config.gamma, config.lr, config.hidden_size, device, config.isDiscrete)\n",
        "    else:\n",
        "        agent = FQIAgent(env.observation_space.shape[0], env.action_space.n, config.tau, config.gamma, config.lr, config.hidden_size, device, config.isDiscrete)\n",
        "    for i in range(config.steps):\n",
        "        loss = agent.learn(buffer.sample_continuous())\n",
        "        losses.append(loss)     \n",
        "        if i % 1000 == 0:\n",
        "            print(f\"[TRAIN {i}] Mean Q Loss: {np.mean(losses)} \")\n",
        "    \n",
        "    agent.save(config, save_name=\"FQI-DQN\")\n",
        "    \n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(config, env, model):\n",
        "    total_rewards = []\n",
        "    num_episodes = config.episodes\n",
        "    for i in range(num_episodes):\n",
        "        print(f\"[Episódio {i}]\")\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor([state], dtype=torch.long)\n",
        "            state_tensor = to_one_hot(model.state_size, state_tensor)[0].float().unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state_tensor)\n",
        "                action = torch.argmax(q_values).item()\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            step += 1\n",
        "            if step == 10000:\n",
        "                break\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    print(f\"Média de recompensa após {num_episodes} episódios: {sum(total_rewards) / num_episodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para extrair a tabela Q da rede neural\n",
        "def extract_q_table(q_network, num_states, num_actions):\n",
        "    q_table = np.zeros((num_states, num_actions))\n",
        "    for state in range(num_states):\n",
        "        state_tensor = torch.tensor([state], dtype=torch.long)\n",
        "        state_tensor = to_one_hot(num_states, state_tensor)[0].float().unsqueeze(0)\n",
        "        q_values = q_network(state_tensor).detach().numpy()\n",
        "        q_table[state] = q_values\n",
        "    return q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from util.qtable_helper import record_video_qtable\n",
        "from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_and_display(model, env_name):\n",
        "    # Extrair a tabela Q da rede neural treinada\n",
        "    q_table = extract_q_table(model, model.state_size, model.action_size)\n",
        "\n",
        "    # Gravar um vídeo da política treinada\n",
        "    video_path = 'videos/'  # Pasta onde os vídeos serão salvos\n",
        "    video_prefix = f\"fqi_{env_name}\"  # Prefixo para o nome do arquivo de vídeo\n",
        "\n",
        "    # Gravar o vídeo\n",
        "    record_video_qtable(env_name, q_table, episodes=2, folder=video_path, prefix=video_prefix)\n",
        "\n",
        "    # Exibir o vídeo gravado\n",
        "    display_videos_from_path(video_path, prefix=video_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_Continuous(config, env, model):\n",
        "    total_rewards = []\n",
        "    num_episodes = config.episodes\n",
        "    for i in range(num_episodes):\n",
        "        print(f\"[Episódio {i}]\")\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        step = 0\n",
        "        while not done:\n",
        "            state_v = torch.tensor(state, dtype=torch.float32)\n",
        "            state_v = state_v.unsqueeze(0)  # Adiciona dimensão de batch como eixo 0 (e.g. transforma uma lista [a,b,c] em [[a,b,c]])\n",
        "            with torch.no_grad():\n",
        "                q_vals_v = model(state_v)\n",
        "                _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            step += 1\n",
        "            if step == 10000:\n",
        "                break\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    print(f\"Média de recompensa após {num_episodes} episódios: {sum(total_rewards) / num_episodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para renderizar o ambiente com o modelo treinado\n",
        "import time\n",
        "def render_environment(model, env_name, device=\"cpu\", episodes=5, max_steps=100000):\n",
        "    env = gym.make(env_name, render_mode=\"human\")  # Modo \"human\" para renderização\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "\n",
        "        while not done and step < max_steps:\n",
        "            # Renderiza o ambiente\n",
        "            env.render()\n",
        "            time.sleep(0.02)  # Adiciona um pequeno delay para visualização\n",
        "\n",
        "            # Converte o estado para um tensor e passa pelo modelo\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state_tensor)\n",
        "                action = torch.argmax(q_values).item()\n",
        "\n",
        "            # Executa a ação no ambiente\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            if done or truncated:\n",
        "                print(f\"Episódio {episode + 1}: Recompensa = {episode_reward}, Passos = {step}\")\n",
        "                break\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 32\n",
        "STEPS = 100_000\n",
        "GAMMA = 0.95 # Fator de desconto\n",
        "TAU = 1e-1      # Taxa de atualização da target_network\n",
        "\n",
        "LEARNING_RATE = 3e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = [64, 32]\n",
        "ISDISCRETE = True\n",
        "# Extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[0][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[0][0], COUNT_EPISODES, SEED, GAMMA, TAU, LEARNING_RATE, STEPS, HIDDEN_SIZE, ISDISCRETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[0][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ENVS_REPLAY_BUFFER[0][1]\n",
        "model = MLP(env.observation_space.n, env.action_space.n, HIDDEN_SIZE)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[0][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_and_display(model, ENVS_REPLAY_BUFFER[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Taxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 256\n",
        "STEPS = 300_000\n",
        "GAMMA = 0.97  # Fator de desconto\n",
        "TAU = 1e-2      # Taxa de atualização da target_network\n",
        "LEARNING_RATE = 5e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = [512, 256]\n",
        "ISDISCRETE = True\n",
        "# Extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[1][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[1][0], COUNT_EPISODES, SEED, GAMMA, TAU, LEARNING_RATE, STEPS, HIDDEN_SIZE, ISDISCRETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ENVS_REPLAY_BUFFER[1][1]\n",
        "model = MLP(env.observation_space.n, env.action_space.n, HIDDEN_SIZE)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[1][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_and_display(model, ENVS_REPLAY_BUFFER[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cliffwalking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 128\n",
        "STEPS = 200_000\n",
        "GAMMA = 0.98  # Fator de desconto\n",
        "TAU = 5e-3      # Taxa de atualização da target_network\n",
        "LEARNING_RATE = 5e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = [256, 128, 64]  # Tamanho da camada oculta\n",
        "ISDISCRETE = True\n",
        "# Extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[2][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[2][0], COUNT_EPISODES, SEED, GAMMA, TAU, LEARNING_RATE, STEPS, HIDDEN_SIZE, ISDISCRETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[2][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ENVS_REPLAY_BUFFER[2][1]\n",
        "model = MLP(env.observation_space.n, env.action_space.n, HIDDEN_SIZE)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[2][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_and_display(model, ENVS_REPLAY_BUFFER[2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 256\n",
        "STEPS = 50_000\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "TAU = 5e-3      # Taxa de atualização da target_network\n",
        "LEARNING_RATE = 1e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = [256, 128]\n",
        "ISDISCRETE = False\n",
        "# Extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[3][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[3][0], COUNT_EPISODES, SEED, GAMMA, TAU, LEARNING_RATE, STEPS, HIDDEN_SIZE, ISDISCRETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[3][2].memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[3][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ENVS_REPLAY_BUFFER[3][1]\n",
        "model = MLP(env.observation_space.shape[0], env.action_space.n, HIDDEN_SIZE)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[3][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_Continuous(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract_and_display(model, ENVS_REPLAY_BUFFER[3][0])\n",
        "render_environment(model, ENVS_REPLAY_BUFFER[3][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LunarLander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 256\n",
        "STEPS = 50_000\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "TAU = 5e-3      # Taxa de atualização da target_network\n",
        "LEARNING_RATE = 1e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = [256, 256]\n",
        "ISDISCRETE = False\n",
        "# Extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[4][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[4][0], COUNT_EPISODES, SEED, GAMMA, TAU, LEARNING_RATE, STEPS, HIDDEN_SIZE, ISDISCRETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[4][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ENVS_REPLAY_BUFFER[4][1]\n",
        "model = MLP(env.observation_space.shape[0], env.action_space.n, HIDDEN_SIZE)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[4][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_Continuous(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract_and_display(model, ENVS_REPLAY_BUFFER[3][0])\n",
        "render_environment(model, ENVS_REPLAY_BUFFER[4][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_PjVle_uaC34"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
