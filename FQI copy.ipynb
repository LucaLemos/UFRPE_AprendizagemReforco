{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalando dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7w0BqRMVqAGK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !git clone https://github.com/LucaLemos/UFRPE_AprendizagemReforco\n",
        "    sys.path.append(\"/content/UFRPE_AprendizagemReforco\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqFlN1Cpqlc",
        "outputId": "dc2f6c29-5837-4551-b9b6-ec11f188bcb0"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "    !pip install gymnasium==1.0.0   # conferir se precisa\n",
        "    #!pip install tianshou # Para criar o Replay_Buffer\n",
        "    #!pip install d3rlpy==2.7.0\n",
        "    # clone repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Criando Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurações\n",
        "DATASET_SIZE = 200_000  # Tamanho do conjunto de dados (replay buffer)\n",
        "LEARNING_RATE = 1e-3  # Taxa de aprendizado para o otimizador\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "BATCH_SIZE = 128  # Tamanho do batch para treinamento da rede neural\n",
        "HIDDEN_SIZE = 128  # Tamanho da camada oculta da rede neural\n",
        "EPSILON = 0.1  # Taxa de exploração (epsilon-greedy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, batch_size, device=\"cpu\"):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.capacity = capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device  # Adiciona o dispositivo (CPU/GPU)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save_config(self, file_path):\n",
        "        \"\"\"Salva a configuração e os dados do buffer em um arquivo JSON.\"\"\"\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        \n",
        "        buffer_data = {\n",
        "            \"buffer_size\": self.capacity,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"device\": self.device,\n",
        "            \"experience_fields\": [\"state\", \"action\", \"reward\", \"next_state\", \"done\"],\n",
        "            \"memory\": [\n",
        "                {\n",
        "                    \"state\": state.tolist() if isinstance(state, np.ndarray) else state,\n",
        "                    \"action\": action,\n",
        "                    \"reward\": reward,\n",
        "                    \"next_state\": next_state.tolist() if isinstance(next_state, np.ndarray) else next_state,\n",
        "                    \"done\": done\n",
        "                }\n",
        "                for state, action, reward, next_state, done in self.buffer\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(buffer_data, f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def load_config(cls, file_path):\n",
        "        \"\"\"Carrega a configuração e os dados do buffer de um arquivo JSON.\"\"\"\n",
        "        with open(file_path, \"r\") as f:\n",
        "            buffer_data = json.load(f)\n",
        "        \n",
        "        buffer_size = buffer_data[\"buffer_size\"]\n",
        "        batch_size = buffer_data[\"batch_size\"]\n",
        "        device = buffer_data.get(\"device\", \"cpu\")  # Padrão para 'cpu' se não estiver presente\n",
        "        experiences = buffer_data[\"memory\"]\n",
        "        \n",
        "        # Criar um novo ReplayBuffer\n",
        "        replay_buffer = cls(buffer_size, batch_size, device)\n",
        "        \n",
        "        # Restaurar os dados do buffer\n",
        "        for data in experiences:\n",
        "            state = np.array(data[\"state\"]) if isinstance(data[\"state\"], list) else data[\"state\"]\n",
        "            action = data[\"action\"]\n",
        "            reward = data[\"reward\"]\n",
        "            next_state = np.array(data[\"next_state\"]) if isinstance(data[\"next_state\"], list) else data[\"next_state\"]\n",
        "            done = data[\"done\"]\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "        \n",
        "        return replay_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementação do DQNAgent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, hidden_size=128, device=\"cpu\"):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.device = device\n",
        "\n",
        "        # Rede neural para aproximar a função Q\n",
        "        self.q_network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim)\n",
        "        ).to(device)\n",
        "\n",
        "        # Otimizador\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state, epsilon=0.1):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)  # Exploração\n",
        "        else:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state_tensor)\n",
        "            return torch.argmax(q_values).item()  # Exploração\n",
        "\n",
        "    def update(self, batch):\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "        # Converte para tensores\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Calcula os Q-values atuais\n",
        "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Calcula os Q-values do próximo estado\n",
        "        next_q_values = self.q_network(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        # Calcula a perda (MSE)\n",
        "        loss = nn.functional.mse_loss(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para coletar dados\n",
        "def collect_data_with_dqn(env, replay_buffer, dataset_size, lr, gamma, hidden_size=128):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    agent = DQNAgent(\n",
        "        state_dim=env.observation_space.shape[0],\n",
        "        action_dim=env.action_space.n,\n",
        "        lr=lr,\n",
        "        gamma=gamma,\n",
        "        hidden_size=hidden_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    for step in range(dataset_size):\n",
        "        # Escolhe uma ação usando a política epsilon-greedy\n",
        "        action = agent.act(state, epsilon=EPSILON)\n",
        "\n",
        "        # Executa a ação no ambiente\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Armazena a transição no replay buffer\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        # Atualiza o estado\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Se o episódio terminar, reinicia o ambiente\n",
        "        if done or truncated:\n",
        "            state, _ = env.reset()\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "\n",
        "        # Treina o agente periodicamente\n",
        "        if len(replay_buffer) > BATCH_SIZE:\n",
        "            batch = replay_buffer.sample(BATCH_SIZE)\n",
        "            agent.update(batch)\n",
        "\n",
        "    return episode_rewards, agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset gerado para CartPole-v1 com 200000 transições.\n",
            "Dataset gerado para LunarLander-v3 com 200000 transições.\n"
          ]
        }
      ],
      "source": [
        "# Passo 1: Coletar dados usando DQN\n",
        "ENV_NAMES = [\"CartPole-v1\", \"LunarLander-v3\"]\n",
        "ENVS_REPLAY_BUFFER = []\n",
        "\n",
        "for env_name in ENV_NAMES:\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    replay_buffer = ReplayBuffer(DATASET_SIZE, BATCH_SIZE)\n",
        "    episode_rewards, agent = collect_data_with_dqn(env, replay_buffer, DATASET_SIZE, LEARNING_RATE, GAMMA)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer, episode_rewards, agent))\n",
        "\n",
        "    # Salva o replay buffer\n",
        "    replay_buffer.save_config(f\"config/dataset/dqn/{env_name}.json\")\n",
        "    print(f\"Dataset gerado para {env_name} com {len(replay_buffer)} transições.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinando o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carregando Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from util.network import ReplayBuffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = [\"CartPole-v1\", \"LunarLander-v3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER = []\n",
        "for env_name in ENV_NAMES:\n",
        "    replay_buffer = ReplayBuffer.load_config(f\"config\\dataset\\dqn\\{env_name}.json\")\n",
        "    env = gym.make(env_name)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definições de Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = [\"CartPole-v1\", \"LunarLander-v3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_config(env_name, count_episodes, seed, gamma, tau, alpha, lr, steps, hidden_size=256):\n",
        "    parser = argparse.ArgumentParser(description='RL')\n",
        "    parser.add_argument(\"--run_name\", type=str, default=f\"{env_name}-DQN-FQI\", help=\"Run name\")\n",
        "    parser.add_argument(\"--env\", type=str, default=env_name, help=\"Gym environment name\")\n",
        "    parser.add_argument(\"--episodes\", type=int, default=count_episodes, help=\"Number of episodes\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=seed, help=\"Random seed\")\n",
        "    parser.add_argument(\"--steps\", type=int, default=steps, help=\"Training steps\")\n",
        "    \n",
        "    parser.add_argument(\"--gamma\", type=float, default=gamma, help=\"Discount factor\")\n",
        "    parser.add_argument(\"--tau\", type=float, default=tau, help=\"Target network update rate\")\n",
        "    parser.add_argument(\"--alpha\", type=float, default=alpha, help=\"CQL regularization weight\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=lr, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=hidden_size, help=\"Size of hidden layers\")\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "from collections import deque\n",
        "from util.network import CQLAgent, save, to_one_hot\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agente CQL\n",
        "class CQLAgent:\n",
        "    def __init__(self, state_size, action_size, tau, gamma, lr, alpha, model, hidden_size, device):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "        self.network = model(state_size, action_size, hidden_size).to(device)\n",
        "        self.target_net = model(state_size, action_size, hidden_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "\n",
        "    def FQIlearn(self, experiences):\n",
        "        self.network.train()\n",
        "    \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "    \n",
        "        # Converte para tensores\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)  # Índices inteiros\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
        "    \n",
        "        # Verifique a forma de actions\n",
        "        if actions.dim() == 1:\n",
        "            actions = actions.unsqueeze(1)  # Transforma para (batch_size, 1)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            # Calcula Q_targets apenas com a equação de Bellman\n",
        "            next_q_values = self.target_net(next_states).max(1)[0]\n",
        "            q_targets = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "    \n",
        "        # Calcula Q_values atuais\n",
        "        q_values = self.network(states).gather(1, actions)  # actions já está na forma correta\n",
        "    \n",
        "        # Calcula a perda\n",
        "        loss = nn.functional.mse_loss(q_values.squeeze(), q_targets)\n",
        "    \n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_DQN_FQI(config, buffer, model_class):\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "\n",
        "    env = gym.make(config.env)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    agent = CQLAgent(env.observation_space.shape[0], env.action_space.n, config.tau, config.gamma, config.lr, \n",
        "                     config.alpha, model_class, config.hidden_size, device)\n",
        "\n",
        "    for i in range(config.steps):\n",
        "        bellmann_error = agent.FQIlearn(buffer.sample())\n",
        "        agent.alpha = config.alpha * np.exp(-3e-6 * i)  # Atualização do peso CQL           \n",
        "        if i % 1000 == 0:\n",
        "            print(f\"[TRAIN {i}] Q Loss: {bellmann_error} \")\n",
        "    \n",
        "    save(config, save_name=\"FQI-DQN\", model=agent.network, wandb=wandb, ep=i)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(config, env, model):\n",
        "    total_rewards = []\n",
        "    num_episodes = config.episodes\n",
        "    for i in range(num_episodes):\n",
        "        print(f\"[Episódio {i}]\")\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state_tensor)\n",
        "                action = torch.argmax(q_values).item()\n",
        "\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            step += 1\n",
        "            if step == 10000:\n",
        "                break\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    print(f\"Média de recompensa após {num_episodes} episódios: {sum(total_rewards) / num_episodes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para extrair a tabela Q da rede neural\n",
        "def extract_q_table(q_network, num_states, num_actions):\n",
        "    q_table = np.zeros((num_states, num_actions))\n",
        "    for state in range(num_states):\n",
        "        state_tensor = torch.tensor([state], dtype=torch.long)\n",
        "        state_tensor = to_one_hot(state_tensor, q_network.input_size)[0].float().unsqueeze(0)\n",
        "        q_values = q_network(state_tensor).detach().numpy()\n",
        "        q_table[state] = q_values\n",
        "    return q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "from util.qtable_helper import record_video_qtable\n",
        "from util.notebook import display_videos_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_and_display(model, env_name):\n",
        "    # Extrair a tabela Q da rede neural treinada\n",
        "    q_table = extract_q_table(model, model.input_size, model.action_size)\n",
        "\n",
        "    # Gravar um vídeo da política treinada\n",
        "    video_path = 'videos/'  # Pasta onde os vídeos serão salvos\n",
        "    video_prefix = f\"fqi_{env_name}\"  # Prefixo para o nome do arquivo de vídeo\n",
        "\n",
        "    # Gravar o vídeo\n",
        "    record_video_qtable(env_name, q_table, episodes=2, folder=video_path, prefix=video_prefix)\n",
        "\n",
        "    # Exibir o vídeo gravado\n",
        "    display_videos_from_path(video_path, prefix=video_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "CartPole = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 64\n",
        "STEPS = 100_000\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "TAU = 5e-3      # Taxa de atualização da target_network\n",
        "ALPHA = 1      # Peso do termo CQL\n",
        "LEARNING_RATE = 3e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[0][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[0][0], COUNT_EPISODES, SEED, GAMMA, TAU, ALPHA, LEARNING_RATE, STEPS, HIDDEN_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(actions, dtype=torch.long).to(self.device)  # Índices inteiros\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:41: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(q_values.squeeze(), q_targets)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN 0] Q Loss: 1.4104034900665283 \n",
            "[TRAIN 1000] Q Loss: 0.00010075777390738949 \n",
            "[TRAIN 2000] Q Loss: 4.388205707073212e-05 \n",
            "[TRAIN 3000] Q Loss: 3.053560067201033e-05 \n",
            "[TRAIN 4000] Q Loss: 3.1759442208567634e-05 \n",
            "[TRAIN 5000] Q Loss: 1.4607575394620653e-05 \n",
            "[TRAIN 6000] Q Loss: 1.0782925528474152e-05 \n",
            "[TRAIN 7000] Q Loss: 2.5532979634590447e-05 \n",
            "[TRAIN 8000] Q Loss: 2.376675183768384e-05 \n",
            "[TRAIN 9000] Q Loss: 2.4542772735003382e-05 \n",
            "[TRAIN 10000] Q Loss: 3.270918023190461e-05 \n",
            "[TRAIN 11000] Q Loss: 1.038076061377069e-05 \n",
            "[TRAIN 12000] Q Loss: 2.5780833311728202e-05 \n",
            "[TRAIN 13000] Q Loss: 2.992337613250129e-05 \n",
            "[TRAIN 14000] Q Loss: 4.624911980499746e-06 \n",
            "[TRAIN 15000] Q Loss: 0.0001906540128402412 \n",
            "[TRAIN 16000] Q Loss: 6.242692961677676e-06 \n",
            "[TRAIN 17000] Q Loss: 6.4689638747950085e-06 \n",
            "[TRAIN 18000] Q Loss: 3.158137042191811e-05 \n",
            "[TRAIN 19000] Q Loss: 1.2559566130221356e-05 \n",
            "[TRAIN 20000] Q Loss: 6.218089765752666e-06 \n",
            "[TRAIN 21000] Q Loss: 2.0343128198874183e-05 \n",
            "[TRAIN 22000] Q Loss: 1.6200376194319688e-05 \n",
            "[TRAIN 23000] Q Loss: 0.00017553205543663353 \n",
            "[TRAIN 24000] Q Loss: 2.988991946040187e-05 \n",
            "[TRAIN 25000] Q Loss: 1.1876573807967361e-05 \n",
            "[TRAIN 26000] Q Loss: 0.00016919300833251327 \n",
            "[TRAIN 27000] Q Loss: 2.016500184254255e-05 \n",
            "[TRAIN 28000] Q Loss: 1.2925132068630774e-05 \n",
            "[TRAIN 29000] Q Loss: 1.4860764167679008e-05 \n",
            "[TRAIN 30000] Q Loss: 3.1298535759560764e-05 \n",
            "[TRAIN 31000] Q Loss: 9.293178663938306e-06 \n",
            "[TRAIN 32000] Q Loss: 1.4783794540562667e-05 \n",
            "[TRAIN 33000] Q Loss: 1.3938727533968631e-05 \n",
            "[TRAIN 34000] Q Loss: 8.186294508050196e-06 \n",
            "[TRAIN 35000] Q Loss: 6.467763341788668e-06 \n",
            "[TRAIN 36000] Q Loss: 1.8363525668974034e-05 \n",
            "[TRAIN 37000] Q Loss: 6.149833097879309e-06 \n",
            "[TRAIN 38000] Q Loss: 8.932719538279343e-06 \n",
            "[TRAIN 39000] Q Loss: 1.008247727440903e-05 \n",
            "[TRAIN 40000] Q Loss: 7.024803380772937e-06 \n",
            "[TRAIN 41000] Q Loss: 5.12194992552395e-06 \n",
            "[TRAIN 42000] Q Loss: 9.315725947089959e-06 \n",
            "[TRAIN 43000] Q Loss: 5.5500113376183435e-06 \n",
            "[TRAIN 44000] Q Loss: 4.533731498668203e-06 \n",
            "[TRAIN 45000] Q Loss: 0.0001801177568268031 \n",
            "[TRAIN 46000] Q Loss: 6.4039254539238755e-06 \n",
            "[TRAIN 47000] Q Loss: 4.8786278057377785e-06 \n",
            "[TRAIN 48000] Q Loss: 8.546627213945612e-06 \n",
            "[TRAIN 49000] Q Loss: 2.319753184565343e-05 \n",
            "[TRAIN 50000] Q Loss: 4.5275855882209726e-06 \n",
            "[TRAIN 51000] Q Loss: 5.751165190304164e-06 \n",
            "[TRAIN 52000] Q Loss: 2.949118197648204e-06 \n",
            "[TRAIN 53000] Q Loss: 4.639355211111251e-06 \n",
            "[TRAIN 54000] Q Loss: 4.308674306230387e-06 \n",
            "[TRAIN 55000] Q Loss: 3.912532065442065e-06 \n",
            "[TRAIN 56000] Q Loss: 0.00016230985056608915 \n",
            "[TRAIN 57000] Q Loss: 1.2609706573130097e-05 \n",
            "[TRAIN 58000] Q Loss: 3.906574420398101e-06 \n",
            "[TRAIN 59000] Q Loss: 2.225803655164782e-06 \n",
            "[TRAIN 60000] Q Loss: 1.996033233808703e-06 \n",
            "[TRAIN 61000] Q Loss: 6.208495051396312e-06 \n",
            "[TRAIN 62000] Q Loss: 2.4380142349400558e-06 \n",
            "[TRAIN 63000] Q Loss: 1.2721152415906545e-05 \n",
            "[TRAIN 64000] Q Loss: 0.0001646903983782977 \n",
            "[TRAIN 65000] Q Loss: 8.581372640037443e-06 \n",
            "[TRAIN 66000] Q Loss: 5.366054665500997e-06 \n",
            "[TRAIN 67000] Q Loss: 4.224524673190899e-06 \n",
            "[TRAIN 68000] Q Loss: 2.9223563615232706e-05 \n",
            "[TRAIN 69000] Q Loss: 3.673263790915371e-06 \n",
            "[TRAIN 70000] Q Loss: 4.32020306107006e-06 \n",
            "[TRAIN 71000] Q Loss: 4.292463563615456e-05 \n",
            "[TRAIN 72000] Q Loss: 4.029177034681197e-06 \n",
            "[TRAIN 73000] Q Loss: 0.00018502783495932817 \n",
            "[TRAIN 74000] Q Loss: 3.0116941616142867e-06 \n",
            "[TRAIN 75000] Q Loss: 4.8394758778158575e-06 \n",
            "[TRAIN 76000] Q Loss: 4.135391918680398e-06 \n",
            "[TRAIN 77000] Q Loss: 6.140257937659044e-06 \n",
            "[TRAIN 78000] Q Loss: 4.614235876942985e-06 \n",
            "[TRAIN 79000] Q Loss: 6.048688192095142e-06 \n",
            "[TRAIN 80000] Q Loss: 0.00017425976693630219 \n",
            "[TRAIN 81000] Q Loss: 2.5785282105061924e-06 \n",
            "[TRAIN 82000] Q Loss: 5.9996727941324934e-06 \n",
            "[TRAIN 83000] Q Loss: 1.2675397556449752e-05 \n",
            "[TRAIN 84000] Q Loss: 2.531505742808804e-06 \n",
            "[TRAIN 85000] Q Loss: 4.738098596135387e-06 \n",
            "[TRAIN 86000] Q Loss: 2.2821629954705713e-06 \n",
            "[TRAIN 87000] Q Loss: 9.085035344469361e-06 \n",
            "[TRAIN 88000] Q Loss: 5.221205356065184e-06 \n",
            "[TRAIN 89000] Q Loss: 4.450302640179871e-06 \n",
            "[TRAIN 90000] Q Loss: 3.525192369124852e-06 \n",
            "[TRAIN 91000] Q Loss: 2.716097014854313e-06 \n",
            "[TRAIN 92000] Q Loss: 2.647984274517512e-06 \n",
            "[TRAIN 93000] Q Loss: 1.962315991477226e-06 \n",
            "[TRAIN 94000] Q Loss: 5.150172000867315e-06 \n",
            "[TRAIN 95000] Q Loss: 2.955621312139556e-06 \n",
            "[TRAIN 96000] Q Loss: 3.7910631363047287e-06 \n",
            "[TRAIN 97000] Q Loss: 5.7517631830705795e-06 \n",
            "[TRAIN 98000] Q Loss: 3.0550397696060827e-06 \n",
            "[TRAIN 99000] Q Loss: 4.0763588913250715e-06 \n"
          ]
        }
      ],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[0][2], QNetwork)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ENVS_REPLAY_BUFFER[0][1]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork(env.observation_space.shape[0], env.action_space.n, HIDDEN_SIZE).to(device)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[0][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Episódio 0]\n",
            "[Episódio 1]\n",
            "[Episódio 2]\n",
            "[Episódio 3]\n",
            "[Episódio 4]\n",
            "[Episódio 5]\n",
            "[Episódio 6]\n",
            "[Episódio 7]\n",
            "[Episódio 8]\n",
            "[Episódio 9]\n",
            "[Episódio 10]\n",
            "[Episódio 11]\n",
            "[Episódio 12]\n",
            "[Episódio 13]\n",
            "[Episódio 14]\n",
            "[Episódio 15]\n",
            "[Episódio 16]\n",
            "[Episódio 17]\n",
            "[Episódio 18]\n",
            "[Episódio 19]\n",
            "[Episódio 20]\n",
            "[Episódio 21]\n",
            "[Episódio 22]\n",
            "[Episódio 23]\n",
            "[Episódio 24]\n",
            "[Episódio 25]\n",
            "[Episódio 26]\n",
            "[Episódio 27]\n",
            "[Episódio 28]\n",
            "[Episódio 29]\n",
            "[Episódio 30]\n",
            "[Episódio 31]\n",
            "[Episódio 32]\n",
            "[Episódio 33]\n",
            "[Episódio 34]\n",
            "[Episódio 35]\n",
            "[Episódio 36]\n",
            "[Episódio 37]\n",
            "[Episódio 38]\n",
            "[Episódio 39]\n",
            "[Episódio 40]\n",
            "[Episódio 41]\n",
            "[Episódio 42]\n",
            "[Episódio 43]\n",
            "[Episódio 44]\n",
            "[Episódio 45]\n",
            "[Episódio 46]\n",
            "[Episódio 47]\n",
            "[Episódio 48]\n",
            "[Episódio 49]\n",
            "Média de recompensa após 50 episódios: 9.36\n"
          ]
        }
      ],
      "source": [
        "evaluate(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'QNetwork' object has no attribute 'input_size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[206], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mextract_and_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENVS_REPLAY_BUFFER\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[187], line 3\u001b[0m, in \u001b[0;36mextract_and_display\u001b[1;34m(model, env_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_and_display\u001b[39m(model, env_name):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Extrair a tabela Q da rede neural treinada\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     q_table \u001b[38;5;241m=\u001b[39m extract_q_table(model, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m, model\u001b[38;5;241m.\u001b[39maction_size)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Gravar um vídeo da política treinada\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideos/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Pasta onde os vídeos serão salvos\u001b[39;00m\n",
            "File \u001b[1;32md:\\Downloads_D\\Faculdade\\git\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'QNetwork' object has no attribute 'input_size'"
          ]
        }
      ],
      "source": [
        "extract_and_display(model, ENVS_REPLAY_BUFFER[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LunarLander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "CartPole = gym.make(\"LunarLander-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "COUNT_EPISODES = 50\n",
        "SEED = 777\n",
        "BATCH_SIZE = 64\n",
        "STEPS = 100_000\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "TAU = 5e-3      # Taxa de atualização da target_network\n",
        "ALPHA = 1      # Peso do termo CQL\n",
        "LEARNING_RATE = 3e-4  # Taxa de aprendizado para o otimizador\n",
        "HIDDEN_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER[1][2].batch_size = BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENVS_REPLAY_BUFFER[1][0], COUNT_EPISODES, SEED, GAMMA, TAU, ALPHA, LEARNING_RATE, STEPS, HIDDEN_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(actions, dtype=torch.long).to(self.device)  # Índices inteiros\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
            "C:\\Users\\lucam\\AppData\\Local\\Temp\\ipykernel_19984\\1777478169.py:41: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(q_values.squeeze(), q_targets)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN 0] Q Loss: 11.888727188110352 \n",
            "[TRAIN 1000] Q Loss: 4.2599029541015625 \n",
            "[TRAIN 2000] Q Loss: 6.815374851226807 \n",
            "[TRAIN 3000] Q Loss: 16.661781311035156 \n",
            "[TRAIN 4000] Q Loss: 159.94540405273438 \n",
            "[TRAIN 5000] Q Loss: 4.9422831535339355 \n",
            "[TRAIN 6000] Q Loss: 7.260927200317383 \n",
            "[TRAIN 7000] Q Loss: 6.917390823364258 \n",
            "[TRAIN 8000] Q Loss: 5.228265762329102 \n",
            "[TRAIN 9000] Q Loss: 3.0697104930877686 \n",
            "[TRAIN 10000] Q Loss: 10.500244140625 \n",
            "[TRAIN 11000] Q Loss: 4.532662868499756 \n",
            "[TRAIN 12000] Q Loss: 5.676116943359375 \n",
            "[TRAIN 13000] Q Loss: 9.277758598327637 \n",
            "[TRAIN 14000] Q Loss: 6.5929718017578125 \n",
            "[TRAIN 15000] Q Loss: 6.247354507446289 \n",
            "[TRAIN 16000] Q Loss: 6.98267126083374 \n",
            "[TRAIN 17000] Q Loss: 164.92010498046875 \n",
            "[TRAIN 18000] Q Loss: 5.616073131561279 \n",
            "[TRAIN 19000] Q Loss: 6.585763931274414 \n",
            "[TRAIN 20000] Q Loss: 3.6890101432800293 \n",
            "[TRAIN 21000] Q Loss: 10.306714057922363 \n",
            "[TRAIN 22000] Q Loss: 10.839824676513672 \n",
            "[TRAIN 23000] Q Loss: 6.596324920654297 \n",
            "[TRAIN 24000] Q Loss: 9.16485595703125 \n",
            "[TRAIN 25000] Q Loss: 4.32943058013916 \n",
            "[TRAIN 26000] Q Loss: 159.56517028808594 \n",
            "[TRAIN 27000] Q Loss: 9.431050300598145 \n",
            "[TRAIN 28000] Q Loss: 13.662206649780273 \n",
            "[TRAIN 29000] Q Loss: 9.292381286621094 \n",
            "[TRAIN 30000] Q Loss: 4.114826679229736 \n",
            "[TRAIN 31000] Q Loss: 4.569766998291016 \n",
            "[TRAIN 32000] Q Loss: 3.954525947570801 \n",
            "[TRAIN 33000] Q Loss: 6.200333118438721 \n",
            "[TRAIN 34000] Q Loss: 13.462442398071289 \n",
            "[TRAIN 35000] Q Loss: 9.001832962036133 \n",
            "[TRAIN 36000] Q Loss: 17.655704498291016 \n",
            "[TRAIN 37000] Q Loss: 164.42822265625 \n",
            "[TRAIN 38000] Q Loss: 6.715891361236572 \n",
            "[TRAIN 39000] Q Loss: 10.167183876037598 \n",
            "[TRAIN 40000] Q Loss: 11.713264465332031 \n",
            "[TRAIN 41000] Q Loss: 16.665294647216797 \n",
            "[TRAIN 42000] Q Loss: 8.675468444824219 \n",
            "[TRAIN 43000] Q Loss: 7.184548854827881 \n",
            "[TRAIN 44000] Q Loss: 2.292721748352051 \n",
            "[TRAIN 45000] Q Loss: 6.701523303985596 \n",
            "[TRAIN 46000] Q Loss: 7.633748531341553 \n",
            "[TRAIN 47000] Q Loss: 161.11805725097656 \n",
            "[TRAIN 48000] Q Loss: 27.07084083557129 \n",
            "[TRAIN 49000] Q Loss: 8.553418159484863 \n",
            "[TRAIN 50000] Q Loss: 11.553322792053223 \n",
            "[TRAIN 51000] Q Loss: 324.6888732910156 \n",
            "[TRAIN 52000] Q Loss: 9.306105613708496 \n",
            "[TRAIN 53000] Q Loss: 7.324859142303467 \n",
            "[TRAIN 54000] Q Loss: 12.025064468383789 \n",
            "[TRAIN 55000] Q Loss: 3.0733065605163574 \n",
            "[TRAIN 56000] Q Loss: 170.99990844726562 \n",
            "[TRAIN 57000] Q Loss: 6.684576511383057 \n",
            "[TRAIN 58000] Q Loss: 2.7163448333740234 \n",
            "[TRAIN 59000] Q Loss: 6.232741832733154 \n",
            "[TRAIN 60000] Q Loss: 6.073822498321533 \n",
            "[TRAIN 61000] Q Loss: 2.190432071685791 \n",
            "[TRAIN 62000] Q Loss: 20.738937377929688 \n",
            "[TRAIN 63000] Q Loss: 2.6069324016571045 \n",
            "[TRAIN 64000] Q Loss: 4.617377281188965 \n",
            "[TRAIN 65000] Q Loss: 2.7023634910583496 \n",
            "[TRAIN 66000] Q Loss: 5.843705177307129 \n",
            "[TRAIN 67000] Q Loss: 5.65179967880249 \n",
            "[TRAIN 68000] Q Loss: 4.493399143218994 \n",
            "[TRAIN 69000] Q Loss: 4.441455841064453 \n",
            "[TRAIN 70000] Q Loss: 5.871248245239258 \n",
            "[TRAIN 71000] Q Loss: 10.970827102661133 \n",
            "[TRAIN 72000] Q Loss: 39.121673583984375 \n",
            "[TRAIN 73000] Q Loss: 10.750022888183594 \n",
            "[TRAIN 74000] Q Loss: 4.005951881408691 \n",
            "[TRAIN 75000] Q Loss: 7.509309768676758 \n",
            "[TRAIN 76000] Q Loss: 2.9795002937316895 \n",
            "[TRAIN 77000] Q Loss: 9.129161834716797 \n",
            "[TRAIN 78000] Q Loss: 1.9478007555007935 \n",
            "[TRAIN 79000] Q Loss: 4.49527645111084 \n",
            "[TRAIN 80000] Q Loss: 4.755409240722656 \n",
            "[TRAIN 81000] Q Loss: 6.430188179016113 \n",
            "[TRAIN 82000] Q Loss: 13.98558521270752 \n",
            "[TRAIN 83000] Q Loss: 9.19053840637207 \n",
            "[TRAIN 84000] Q Loss: 161.01800537109375 \n",
            "[TRAIN 85000] Q Loss: 3.5281741619110107 \n",
            "[TRAIN 86000] Q Loss: 10.0446138381958 \n",
            "[TRAIN 87000] Q Loss: 9.051559448242188 \n",
            "[TRAIN 88000] Q Loss: 6.939279556274414 \n",
            "[TRAIN 89000] Q Loss: 162.4959716796875 \n",
            "[TRAIN 90000] Q Loss: 3.7315995693206787 \n",
            "[TRAIN 91000] Q Loss: 6.303199291229248 \n",
            "[TRAIN 92000] Q Loss: 163.1887664794922 \n",
            "[TRAIN 93000] Q Loss: 314.4100036621094 \n",
            "[TRAIN 94000] Q Loss: 5.797959327697754 \n",
            "[TRAIN 95000] Q Loss: 8.861741065979004 \n",
            "[TRAIN 96000] Q Loss: 8.524923324584961 \n",
            "[TRAIN 97000] Q Loss: 5.804880619049072 \n",
            "[TRAIN 98000] Q Loss: 6.469918251037598 \n",
            "[TRAIN 99000] Q Loss: 7.607137680053711 \n"
          ]
        }
      ],
      "source": [
        "env = train_DQN_FQI(config, ENVS_REPLAY_BUFFER[1][2], QNetwork)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ENVS_REPLAY_BUFFER[1][1]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork(env.observation_space.shape[0], env.action_space.n, HIDDEN_SIZE).to(device)\n",
        "model.load_state_dict(torch.load(f\"trained_models/{ENVS_REPLAY_BUFFER[1][0]}FQI-DQN.pth\"))\n",
        "model.eval()  # Definir para modo de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Episódio 0]\n",
            "[Episódio 1]\n",
            "[Episódio 2]\n",
            "[Episódio 3]\n",
            "[Episódio 4]\n",
            "[Episódio 5]\n",
            "[Episódio 6]\n",
            "[Episódio 7]\n",
            "[Episódio 8]\n",
            "[Episódio 9]\n",
            "[Episódio 10]\n",
            "[Episódio 11]\n",
            "[Episódio 12]\n",
            "[Episódio 13]\n",
            "[Episódio 14]\n",
            "[Episódio 15]\n",
            "[Episódio 16]\n",
            "[Episódio 17]\n",
            "[Episódio 18]\n",
            "[Episódio 19]\n",
            "[Episódio 20]\n",
            "[Episódio 21]\n",
            "[Episódio 22]\n",
            "[Episódio 23]\n",
            "[Episódio 24]\n",
            "[Episódio 25]\n",
            "[Episódio 26]\n",
            "[Episódio 27]\n",
            "[Episódio 28]\n",
            "[Episódio 29]\n",
            "[Episódio 30]\n",
            "[Episódio 31]\n",
            "[Episódio 32]\n",
            "[Episódio 33]\n",
            "[Episódio 34]\n",
            "[Episódio 35]\n",
            "[Episódio 36]\n",
            "[Episódio 37]\n",
            "[Episódio 38]\n",
            "[Episódio 39]\n",
            "[Episódio 40]\n",
            "[Episódio 41]\n",
            "[Episódio 42]\n",
            "[Episódio 43]\n",
            "[Episódio 44]\n",
            "[Episódio 45]\n",
            "[Episódio 46]\n",
            "[Episódio 47]\n",
            "[Episódio 48]\n",
            "[Episódio 49]\n",
            "Média de recompensa após 50 episódios: -280.402192002825\n"
          ]
        }
      ],
      "source": [
        "evaluate(config, env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'QNetwork' object has no attribute 'input_size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[216], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mextract_and_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENVS_REPLAY_BUFFER\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[187], line 3\u001b[0m, in \u001b[0;36mextract_and_display\u001b[1;34m(model, env_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_and_display\u001b[39m(model, env_name):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Extrair a tabela Q da rede neural treinada\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     q_table \u001b[38;5;241m=\u001b[39m extract_q_table(model, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m, model\u001b[38;5;241m.\u001b[39maction_size)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Gravar um vídeo da política treinada\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideos/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Pasta onde os vídeos serão salvos\u001b[39;00m\n",
            "File \u001b[1;32md:\\Downloads_D\\Faculdade\\git\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'QNetwork' object has no attribute 'input_size'"
          ]
        }
      ],
      "source": [
        "extract_and_display(model, ENVS_REPLAY_BUFFER[1][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_PjVle_uaC34"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
