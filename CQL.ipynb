{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalando dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w0BqRMVqAGK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !git clone https://github.com/LucaLemos/UFRPE_AprendizagemReforco\n",
        "    sys.path.append(\"/content/UFRPE_AprendizagemReforco\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSqFlN1Cpqlc",
        "outputId": "dc2f6c29-5837-4551-b9b6-ec11f188bcb0"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "    !pip install gymnasium==1.0.0   # conferir se precisa\n",
        "    #!pip install tianshou # Para criar o Replay_Buffer\n",
        "    #!pip install d3rlpy==2.7.0\n",
        "    # clone repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Criando Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from util.algorithms import run_sarsa\n",
        "from util.network import ReplayBuffer\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_SIZE = 200_000  # Tamanho do conjunto de dados (replay buffer)\n",
        "LEARNING_RATE = 1e-3  # Taxa de aprendizado para o otimizador\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "BATCH_SIZE = 128  # Tamanho do batch para treinamento da rede neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 1: Coletar um conjunto fixo de transições (Replay Buffer)\n",
        "ENV_NAMES = [\"FrozenLake-v1\", \"Taxi-v3\", \"CliffWalking-v0\"]\n",
        "ENVS_REPLAY_BUFFER = []\n",
        "for i, env_name in enumerate(ENV_NAMES):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    replay_buffer = ReplayBuffer(DATASET_SIZE, BATCH_SIZE, device)\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    \n",
        "    sum_rewards_per_ep, q = run_sarsa(env, replay_buffer, DATASET_SIZE, LEARNING_RATE, GAMMA)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer, sum_rewards_per_ep, q))\n",
        "    replay_buffer.save_config(f\"config\\dataset\\sarsa\\{env_name}.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from util.network import ReplayBuffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = [\"FrozenLake-v1\", \"Taxi-v3\", \"CliffWalking-v0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENVS_REPLAY_BUFFER = []\n",
        "for env_name in ENV_NAMES:\n",
        "    replay_buffer = ReplayBuffer.load_config(f\"config\\dataset\\sarsa\\{env_name}.json\")\n",
        "    env = gym.make(env_name)\n",
        "    ENVS_REPLAY_BUFFER.append((env_name, env, replay_buffer))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV_NAMES = [\"FrozenLake-v1\", \"Taxi-v3\", \"CliffWalking-v0\"]\n",
        "COUNT_EPISODES = 500\n",
        "BUFFER_SIZE = 200_000\n",
        "SEED = 777\n",
        "MIN_EPS = 1e-2\n",
        "EPS_FRAMES = 1e4\n",
        "LOG_VIDEO = 0\n",
        "SAVE_EVERY = 100\n",
        "\n",
        "# Hiperparâmetros\n",
        "LEARNING_RATE = 1e-3  # Taxa de aprendizado para o otimizador\n",
        "GAMMA = 0.99  # Fator de desconto\n",
        "TAU = 1e-3      # Taxa de atualização da target_network\n",
        "ALPHA = 0.5      # Peso do termo CQL\n",
        "# Extra: \n",
        "TARGET_UPDATE_FREQ = 200  # Atualizar a target network a cada 2 iterações\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_config(env_name, count_episodes, buffer_size, seed, min_eps, eps_frames, log_video, save_every, gamma, tau, alpha, target_update_freq, lr):\n",
        "    parser = argparse.ArgumentParser(description='RL')\n",
        "    parser.add_argument(\"--run_name\", type=str, default=f\"{env_name}-DQN-CQL\", help=\"Run name, default: CQL-DQN\")\n",
        "    parser.add_argument(\"--env\", type=str, default=env_name, help=\"Gym environment name, default: CartPole-v0\")\n",
        "    parser.add_argument(\"--episodes\", type=int, default=count_episodes, help=\"Number of episodes, default: 200\")\n",
        "    parser.add_argument(\"--buffer_size\", type=int, default=buffer_size, help=\"Maximal training dataset size, default: 100_000\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=seed, help=\"Seed, default: 1\")\n",
        "    parser.add_argument(\"--min_eps\", type=float, default=min_eps, help=\"Minimal Epsilon, default: 4\")\n",
        "    parser.add_argument(\"--eps_frames\", type=int, default=eps_frames, help=\"Number of steps for annealing the epsilon value to the min epsilon, default: 1e5\")\n",
        "    parser.add_argument(\"--log_video\", type=int, default=log_video, help=\"Log agent behaviour to wanbd when set to 1, default: 0\")\n",
        "    parser.add_argument(\"--save_every\", type=int, default=save_every, help=\"Saves the network every x epochs, default: 25\")\n",
        "    \n",
        "    parser.add_argument(\"--gamma\", type=float, default=gamma, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--tau\", type=float, default=tau, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--alpha\", type=float, default=alpha, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--target_update_freq\", type=float, default=target_update_freq, help=\"Saves the network every x epochs, default: 25\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=lr, help=\"Saves the network every x epochs, default: 25\")\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_config(ENV_NAMES[1], COUNT_EPISODES, BUFFER_SIZE, SEED, MIN_EPS, EPS_FRAMES, LOG_VIDEO, SAVE_EVERY, GAMMA, TAU, ALPHA, TARGET_UPDATE_FREQ, LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "from collections import deque\n",
        "from util.network import CQLAgent, save, to_one_hot\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_DQN_CQL(config, buffer):\n",
        "    np.random.seed(config.seed)\n",
        "    random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "    env = gym.make(config.env)\n",
        "\n",
        "    #env.seed(config.seed)\n",
        "    #env.action_space.seed(config.seed)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    eps = 1.\n",
        "    d_eps = 1 - config.min_eps\n",
        "    steps = 0\n",
        "    average10 = deque(maxlen=10)\n",
        "    total_steps = 0\n",
        "    \n",
        "    with wandb.init(project=\"CQL\", name=config.run_name, config=config):\n",
        "        \n",
        "        agent = CQLAgent(env.observation_space.n, env.action_space.n, config.tau, config.gamma, config.lr, device=device)\n",
        "\n",
        "        wandb.watch(agent.network, log=\"gradients\", log_freq=10)\n",
        "\n",
        "        #buffer = ReplayBuffer(buffer_size=config.buffer_size, batch_size=32, device=device)\n",
        "        \n",
        "        #collect_random(env=env, dataset=buffer, num_samples=10000)\n",
        "        \n",
        "        if config.log_video:\n",
        "            env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n",
        "\n",
        "        for i in range(1, config.episodes+1):\n",
        "            state, _ = env.reset()\n",
        "            episode_steps = 0\n",
        "            rewards = 0\n",
        "            while True:\n",
        "                action = agent.get_action(state, eps)\n",
        "                next_state, reward, terminated, trunc, _ = env.step(action[0])\n",
        "                done = terminated or trunc\n",
        "                buffer.add(state, action, reward, next_state, done)\n",
        "                \n",
        "                loss, cql_loss, bellmann_error = agent.learn(buffer.sample())\n",
        "                state = next_state\n",
        "                rewards += reward\n",
        "                eps = max(1 - ((steps*d_eps)/config.eps_frames), config.min_eps)\n",
        "                episode_steps += 1\n",
        "                steps += 1\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            average10.append(rewards)\n",
        "            total_steps += episode_steps\n",
        "            print(\"Episode: {} | Reward: {} | Q Loss: {} | Steps: {}\".format(i, rewards, loss, steps,))\n",
        "            \n",
        "            wandb.log({\"Reward\": rewards,\n",
        "                       \"Average10\": np.mean(average10),\n",
        "                       \"Steps\": total_steps,\n",
        "                       \"Q Loss\": loss,\n",
        "                       \"CQL Loss\": cql_loss,\n",
        "                       \"Bellmann error\": bellmann_error,\n",
        "                       \"Steps\": steps,\n",
        "                       \"Epsilon\": eps,\n",
        "                       \"Episode\": i,\n",
        "                       \"Buffer size\": buffer.__len__()})\n",
        "            \"\"\"\n",
        "            if (i %10 == 0) and config.log_video:\n",
        "                \n",
        "                mp4list = glob.glob('video/*.mp4')\n",
        "                if len(mp4list) > 1:\n",
        "                    mp4 = mp4list[-2]\n",
        "                    wandb.log({\"gameplays\": wandb.Video(mp4, caption='episode: '+str(i-10), fps=4, format=\"gif\"), \"Episode\": i})\n",
        "            \"\"\"\n",
        "        save(config, save_name=\"CQL-DQN\", model=agent.network, wandb=wandb, ep=i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_DQN_CQL(config, ENVS_REPLAY_BUFFER[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_one_hot(states, state_size): \n",
        "    one_hot_states = torch.zeros((states.shape[0], state_size), device=states.device)\n",
        "    one_hot_states.scatter_(1, states.long().unsqueeze(1), 1)  \n",
        "    return one_hot_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "states, actions, rewards, next_states, dones = ENVS_REPLAY_BUFFER[0][2].sample()\n",
        "state = states[0]\n",
        "\n",
        "print(next_states)\n",
        "env = ENVS_REPLAY_BUFFER[0][1]\n",
        "print(env.observation_space)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "next_states = to_one_hot(next_states, env.observation_space.n)\n",
        "print(next_states)\n",
        "agent = CQLAgent(env.observation_space.n, env.action_space.n, config.tau, config.gamma, config.lr, hidden_size=128)\n",
        "agent.target_net(next_states)\n",
        "\n",
        "\n",
        "\n",
        "state, _ = env.reset()\n",
        "print(state)\n",
        "state_tensor = torch.tensor([state], dtype=torch.long)  # Criar um tensor de batch_size=1\n",
        "state = torch.from_numpy(to_one_hot(state_tensor, env.observation_space.n)[0]).float().unsqueeze(0).to(agent.device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTOiPk0WyaHZ"
      },
      "outputs": [],
      "source": [
        "def extract_policy(q_network, num_states):\n",
        "    policy = []\n",
        "    for state in range(num_states):\n",
        "        state_tensor = to_one_hot(state, num_states).unsqueeze(0)\n",
        "        q_values = q_network(state_tensor)\n",
        "        policy.append(torch.argmax(q_values).item())\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfiytiX5PkCh"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "success_rates = []\n",
        "q_values_means = []\n",
        "rewards_variances = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X-bzu31PnvD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_PjVle_uaC34"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
