{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7w0BqRMVqAGK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aKTfNBr1y_vY"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        \"\"\"\n",
        "        Inicializa o replay buffer com prioridades.\n",
        "        :param capacity: Capacidade máxima do buffer.\n",
        "        :param alpha: Parâmetro de prioridade (0 < alpha <= 1).\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
        "        self.pos = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "        \"\"\"\n",
        "        Adiciona uma transição ao buffer.\n",
        "        :param transition: Tupla (state, action, reward, next_state, done).\n",
        "        \"\"\"\n",
        "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(transition)\n",
        "        else:\n",
        "            self.buffer[self.pos] = transition\n",
        "        self.priorities[self.pos] = max_priority\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        \"\"\"\n",
        "        Amostra um batch de transições com base nas prioridades.\n",
        "        :param batch_size: Tamanho do batch.\n",
        "        :param beta: Parâmetro de compensação (0 < beta <= 1).\n",
        "        :return: Batch de transições, índices e pesos.\n",
        "        \"\"\"\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            priorities = self.priorities\n",
        "        else:\n",
        "            priorities = self.priorities[:self.pos]\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        return samples, indices, np.array(weights, dtype=np.float32)\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"\n",
        "        Atualiza as prioridades das transições amostradas.\n",
        "        :param indices: Índices das transições.\n",
        "        :param priorities: Novas prioridades.\n",
        "        \"\"\"\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yqr4tGCgykfy"
      },
      "outputs": [],
      "source": [
        "def generate_dataset_with_sarsa(env, buffer_capacity=10000, num_episodes=5000, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1):\n",
        "    \"\"\"\n",
        "    Gera um dataset usando o algoritmo SARSA.\n",
        "    :param env: Ambiente Gymnasium.\n",
        "    :param buffer_capacity: Capacidade do replay buffer.\n",
        "    :param num_episodes: Número de episódios para coleta de dados.\n",
        "    :param alpha: Taxa de aprendizado do SARSA.\n",
        "    :param gamma: Fator de desconto.\n",
        "    :param epsilon: Taxa de exploração inicial.\n",
        "    :param epsilon_decay: Decaimento da taxa de exploração.\n",
        "    :param epsilon_min: Taxa de exploração mínima.\n",
        "    :return: Replay buffer preenchido.\n",
        "    \"\"\"\n",
        "    replay_buffer = PrioritizedReplayBuffer(buffer_capacity)\n",
        "\n",
        "    # Inicializar a tabela Q\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # Escolher a primeira ação (epsilon-greedy)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()  # Exploração\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Explotação\n",
        "\n",
        "        episode_transitions = []  # Armazenar transições do episódio atual\n",
        "\n",
        "        while not done:\n",
        "            # Executar a ação e observar o próximo estado e recompensa\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated\n",
        "\n",
        "            # Escolher a próxima ação (epsilon-greedy)\n",
        "            if np.random.rand() < epsilon:\n",
        "                next_action = env.action_space.sample()  # Exploração\n",
        "            else:\n",
        "                next_action = np.argmax(q_table[next_state])  # Explotação\n",
        "\n",
        "            # Criar a transição\n",
        "            transition = (state, action, reward, next_state, done)\n",
        "            episode_transitions.append(transition)\n",
        "\n",
        "            # Atualizar a tabela Q usando SARSA\n",
        "            td_target = reward + gamma * q_table[next_state][next_action] * (not done)\n",
        "            td_error = td_target - q_table[state][action]\n",
        "            q_table[state][action] += alpha * td_error\n",
        "\n",
        "            # Atualizar o estado e a ação\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "        # Adicionar todas as transições do episódio ao replay buffer\n",
        "        for transition in episode_transitions:\n",
        "            replay_buffer.add(transition)\n",
        "\n",
        "        # Decaimento de epsilon\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    return replay_buffer, q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IWsZ6XDQ7OVT"
      },
      "outputs": [],
      "source": [
        "def validate_dataset(replay_buffer):\n",
        "    \"\"\"\n",
        "    Valida o dataset gerado.\n",
        "    :param replay_buffer: Replay buffer contendo as transições.\n",
        "    \"\"\"\n",
        "    # Verificar o tamanho do dataset\n",
        "    print(f\"Tamanho do dataset: {len(replay_buffer.buffer)}\")\n",
        "\n",
        "    # Verificar a diversidade de estados e ações\n",
        "    states = [t[0] for t in replay_buffer.buffer]\n",
        "    actions = [t[1] for t in replay_buffer.buffer]\n",
        "    rewards = [t[2] for t in replay_buffer.buffer]\n",
        "\n",
        "    print(f\"Estados únicos: {len(np.unique(states))}\")\n",
        "    print(f\"Ações únicas: {len(np.unique(actions))}\")\n",
        "    print(f\"Recompensas únicas: {len(np.unique(rewards))}\")\n",
        "\n",
        "    # Verificar se há transições terminais\n",
        "    terminal_transitions = [t for t in replay_buffer.buffer if t[4]]  # done=True\n",
        "    print(f\"Transições terminais: {len(terminal_transitions)}\")\n",
        "\n",
        "    # Verificar a distribuição de recompensas\n",
        "    print(f\"Recompensa mínima: {np.min(rewards)}\")\n",
        "    print(f\"Recompensa máxima: {np.max(rewards)}\")\n",
        "    print(f\"Recompensa média: {np.mean(rewards)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VnmDUdq3WDc",
        "outputId": "5e1f2e50-31a9-4c43-d18c-bb081cd26393"
      },
      "outputs": [],
      "source": [
        "# Configuração do ambiente\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"rgb_array\")\n",
        "\n",
        "# Gerar o dataset com SARSA\n",
        "replay_buffer, q_table = generate_dataset_with_sarsa(env, buffer_capacity=1000000, num_episodes=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamanho do dataset: 264354\n",
            "Estados únicos: 11\n",
            "Ações únicas: 4\n",
            "Recompensas únicas: 2\n",
            "Transições terminais: 10000\n",
            "Recompensa mínima: 0.0\n",
            "Recompensa máxima: 1.0\n",
            "Recompensa média: 0.013201994295527967\n"
          ]
        }
      ],
      "source": [
        "# Validar o dataset\n",
        "validate_dataset(replay_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replay buffer salvo em dataSet/DataSet_FrozenLake.npy\n"
          ]
        }
      ],
      "source": [
        "# Salvar o dataset\n",
        "dataset_filename = 'dataSet/DataSet_FrozenLake.npy'\n",
        "np.save(dataset_filename, np.array((env, replay_buffer.buffer), dtype=object))\n",
        "print(f\"Replay buffer salvo em {dataset_filename}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_PjVle_uaC34"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
